---
title:  "kaggle MoA 대회 (1)"
excerpt: "필사 진행"

categories:
  - AI
tags:
  - Blog
last_modified_at: 2020-10-23
---

> ## 대회 설명

* 추후 설명 예정

> ## 필사 中 공부 Point

* 필사 kernel은 https://www.kaggle.com/namanj27/new-baseline-pytorch-moa
* 해당 대회는 Feature 수가 900개 이상이어서 PCA 진행
* 다음, Variance 기반 Feature Selection 진행 (Variance Threshold 0.5)
* iterstrat 라이브러리를 통한 Multilabel Stratified KFold 진행
	* 해당 함수 활용시 Multilabel에 대해 Stratified 된 fold들 생성 가능 (sklearn에서 제공 안함)
* BatchNorm은 처음 Input을 표준화해도 학습에 따른 은닉층별 input의 분포가 변경됨으로 인한 문제를 해결하기 위해 나왔다 (torch.nn.BatchNorm1d())
    * 좋은 설명: https://de-novo.org/2018/05/28/batch-normalization-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/
* weight norm은 분포의 널뜀을 해결하기 위해 나왔지만 (BatchNorm처럼) 더욱 학습 속도 빠르게 진행 가능 (torch.utils.weight_norm())
	* 좋은 설명(1): https://github.com/aleju/papers/blob/master/neural-nets/Weight_Normalization.md
	* 좋은 설명(2): https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/
* weight decay는 가중치에 손실함수를 통한 제약을 걸어주는 overfitting 방지 방법
	* 좋은 설명: http://www.gisdeveloper.co.kr/?p=8443
* Lr_scheduler: Learning Rate를 학습상황에 맞게 변경시켜주는 방법
* BCEWithLogitLoss: Binary Cross Entropy 손실함수인데 sigmoid를 바로 앞에서 취해줌으로써 sigmoid 활성함수 필요 없음